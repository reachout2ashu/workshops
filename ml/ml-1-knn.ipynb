{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- MNIST Processing scripts from: https://gist.github.com/akesling/5358964"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import struct\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import requests\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def mkdir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        return\n",
    "    os.mkdir(directory)\n",
    "\n",
    "def download(url, destination):\n",
    "    if os.path.exists(destination):\n",
    "        print \"File {} already exists. Skipping download\".format(destination)\n",
    "        return\n",
    "    \n",
    "    print \"Downloading {} to {}\".format(url, destination)\n",
    "    r = requests.get(url, stream=True)\n",
    "    if r.status_code == 200:\n",
    "        with open(destination, 'wb') as f:\n",
    "            r.raw.decode_content = True\n",
    "            shutil.copyfileobj(r.raw, f)   \n",
    "        print \"Download complete!\"\n",
    "    else:\n",
    "        raise ValueError(\"Got a non-200 response\")\n",
    "\n",
    "def gz_decompress(source, dest):\n",
    "    if os.path.exists(dest):\n",
    "        print \"{} already exists. Skipping decompressing\".format(dest)\n",
    "        return \n",
    "    print \"Decompressing {} to {}\".format(source, dest)\n",
    "    with gzip.open(source, 'rb') as reader, open(dest, \"wb\") as writer:\n",
    "        writer.write(reader.read())\n",
    "    print \"Done!\"\n",
    "        \n",
    "def download_mnist(target_dir=\"./\"):\n",
    "    target_dir = os.path.join(target_dir, \"mnist\")\n",
    "    mkdir(target_dir)\n",
    "    \n",
    "    download(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",  \n",
    "             os.path.join(target_dir, \"train-images-idx3-ubyte.gz\"))\n",
    "    gz_decompress(os.path.join(target_dir, \"train-images-idx3-ubyte.gz\"), \n",
    "                  os.path.join(target_dir, \"train-images-idx3-ubyte\"))\n",
    "    \n",
    "    download(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",  \n",
    "             os.path.join(target_dir, \"train-labels-idx1-ubyte.gz\"))\n",
    "    gz_decompress(os.path.join(target_dir, \"train-labels-idx1-ubyte.gz\"), \n",
    "                  os.path.join(target_dir, \"train-labels-idx1-ubyte\"))\n",
    "    \n",
    "    download(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",  \n",
    "             os.path.join(target_dir, \"t10k-images-idx3-ubyte.gz\"))\n",
    "    gz_decompress(os.path.join(target_dir, \"t10k-images-idx3-ubyte.gz\"), \n",
    "                  os.path.join(target_dir, \"t10k-images-idx3-ubyte\"))\n",
    "    \n",
    "    download(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",  \n",
    "             os.path.join(target_dir, \"t10k-labels-idx1-ubyte.gz\"))\n",
    "    gz_decompress(os.path.join(target_dir, \"t10k-labels-idx1-ubyte.gz\"), \n",
    "                  os.path.join(target_dir, \"t10k-labels-idx1-ubyte\"))\n",
    "\n",
    "def read_mnist_full(dataset = \"training\", path = \"./mnist\", selected_labels=None):\n",
    "    if dataset is \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte')\n",
    "    else:\n",
    "        raise ValueError, \"dataset must be 'testing' or 'training'\"\n",
    "\n",
    "    # Load everything in some numpy arrays\n",
    "    with open(fname_lbl, 'rb') as flbl:\n",
    "        magic, num = struct.unpack(\">II\", flbl.read(8))\n",
    "        lbl = np.fromfile(flbl, dtype=np.int8)\n",
    "    with open(fname_img, 'rb') as fimg:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "        img = np.fromfile(fimg, dtype=np.uint8).reshape(len(lbl), rows, cols)\n",
    "\n",
    "    return lbl, img\n",
    "    \n",
    "def read_mnist(dataset = \"training\", path = \"./mnist\", selected_labels=None):\n",
    "    lbl, img = read_mnist_full(dataset, path, selected_labels)\n",
    "    \n",
    "    get_img = lambda idx: (lbl[idx], img[idx])\n",
    "\n",
    "    # random permutation of indices\n",
    "    p = np.random.permutation(len(lbl))\n",
    "    lbl = lbl[p]\n",
    "    img = img[p]\n",
    "    \n",
    "    # Create an iterator which returns each image in turn\n",
    "    for i in xrange(len(lbl)):\n",
    "        if selected_labels != None and lbl[i] not in selected_labels:\n",
    "            continue\n",
    "        yield get_img(i)\n",
    "        \n",
    "\n",
    "def show_mnist(image, title=None, fig_size=None):\n",
    "    \"\"\"\n",
    "    Render a given numpy.uint8 2D array of pixel data.\n",
    "    \"\"\"\n",
    "    if not fig_size:\n",
    "        fig_size = (5, 5)\n",
    "    fig = plt.figure(figsize=fig_size)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    imgplot = ax.imshow(image, cmap=plt.cm.gray)\n",
    "    imgplot.set_interpolation('nearest')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    #ax.xaxis.set_ticks_position('top')\n",
    "    #ax.yaxis.set_ticks_position('left')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "#### Samarth Bhargav\n",
    "#### https://github.com/samarthbhargav/workshops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "- Machine Learning Concepts\n",
    "- Supervised Learning\n",
    "- Classification and Regression\n",
    "- Visualizing Data with Matplotlib\n",
    "- Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Machine Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Siri / Google Now / Alexa / Cortana\n",
    "\n",
    "#### Speech -> Text -> 'Understand' -> Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Optical Character Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Face recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What *isn't* machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification\n",
    "\n",
    "Examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regression \n",
    "\n",
    "Examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TODO List:\n",
    "- ~~KNN~~\n",
    "- Logistic Regression\n",
    "- Decision Trees\n",
    "- Iris, Titanic, MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The MNIST challenge!\n",
    "\n",
    "##### Time to start coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# download the MNIST data, decompress it\n",
    "download_mnist()\n",
    "\n",
    "# read a couple of sample images\n",
    "sample = 100\n",
    "sample_images = []\n",
    "sample_labels = []\n",
    "mnist_reader = read_mnist()\n",
    "for i in range(sample):\n",
    "    lbl, img = next(mnist_reader)\n",
    "    sample_images.append(img)\n",
    "    sample_labels.append(lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, sample - 1)\n",
    "show_mnist(sample_images[idx],sample_labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# each image is a 28x28 image\n",
    "def plot_n(mult, mnist_reader):\n",
    "    imgs = np.zeros(((28*mult), (28*mult)), dtype=np.uint8)\n",
    "\n",
    "    for i in range(mult):\n",
    "        for j in range(mult):\n",
    "            lbl, img = next(mnist_reader)\n",
    "            imgs[i*28:(i+1)*28, j*28:(j+1)*28] = img\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "imgs = plot_n(20, read_mnist())\n",
    "show_mnist(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# variance in the labels makes the problem non-trivial\n",
    "selected_labels = set([2])\n",
    "imgs = plot_n(20, read_mnist(selected_labels=selected_labels))\n",
    "show_mnist(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "selected_labels = set([8, 9])\n",
    "imgs = plot_n(20, read_mnist(selected_labels=selected_labels))\n",
    "show_mnist(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you solve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the computer see it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# what does the computer 'see'?\n",
    "def ascii_show(image):\n",
    "    for y in image:\n",
    "        row = \"\"\n",
    "        for x in y:\n",
    "            row += '{0: <4}'.format(x)\n",
    "        print row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "idx = random.randint(0, sample - 1)\n",
    "ascii_show(sample_images[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Algorithm : K-Nearest Neighbours\n",
    "\n",
    "Given a sample to classify, we get the K-nearest neighbours. The most represented class in the K-nearest neighbours is the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Aside: How can we compute distance between two vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def plot_points(points, labels=None, xlim=(0, 10), ylim=(0, 10)):\n",
    "    points = np.array(points)\n",
    "    # since it's two dimensions, they can be plotted!\n",
    "    plt.plot(points[:, 0], points[:, 1], \"bo\")\n",
    "    plt.xlim(xlim[0], xlim[1])\n",
    "    plt.ylim(ylim[0], ylim[1])\n",
    "    if labels:\n",
    "        for p, l in zip(points, labels):\n",
    "            plt.text(p[0] + .2, p[1] + 0.1, \"${}$\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's take 3 arrays for example\n",
    "a = np.array([2, 3])\n",
    "b = np.array([3, 7])\n",
    "c = np.array([1, 1])\n",
    "\n",
    "plot_points([a, b, c], [\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# distance metric 1\n",
    "# taxi-cab / manhattan distance\n",
    "Image(url=\"http://www.joachimdespland.com/img/mammoth/pathfinding.fig2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.abs(a - b).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def manhattan(x, y): \n",
    "    # this also works for >2 dimensions\n",
    "    return np.abs(x - y).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# it's symmetric, so we can compute only once\n",
    "manhattan(a, b), manhattan(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print \"Distance between a and b:\", manhattan(a,b)\n",
    "print \"Distance between a and c:\", manhattan(a,c)\n",
    "print \"Distance between b and c:\", manhattan(b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# now let's take a look at the euclidean distance\n",
    "Image(\"http://rosalind.info/media/Euclidean_distance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# it's the 'straight line ' distance between 2 points\n",
    "np.sqrt(((a-b)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def euclidean(x, y):\n",
    "    return np.sqrt(((x-y)**2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# it's symmetric, so we can compute only once\n",
    "euclidean(a, b), euclidean(b, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print \"Distance between a and b:\", euclidean(a,b)\n",
    "print \"Distance between a and c:\", euclidean(a,c)\n",
    "print \"Distance between b and c:\", euclidean(b,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to K-NN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def find_nearest(points, cmp_point, point_index, k, distance_function):\n",
    "    distances = []\n",
    "    for idx, point in enumerate(points):\n",
    "        if idx == point_idx:\n",
    "            continue\n",
    "        distances.append((idx, distance_function(cmp_point, points[idx])))\n",
    "    distances.sort(key=lambda d: d[1])\n",
    "    return distances[:k]\n",
    "\n",
    "a = np.array([2, 3])\n",
    "b = np.array([3, 7])\n",
    "c = np.array([1, 1])\n",
    "\n",
    "points = [a, b, c]\n",
    "point_idx = 2\n",
    "\n",
    "idx, distance = find_nearest(points, points[point_idx], point_idx, 1, euclidean)[0]\n",
    "print \"Point nearest to {} is {}\".format(points[point_idx], points[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# now, let's try out K-NN for the MNIST dataset\n",
    "labels, images = read_mnist_full()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# shape tells us the dimensions of the data\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# since it's a 28x28 matrix, the images have to be reshaped to make it work \n",
    "images = images.reshape(60000, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# we can get back the original data by reshaping it again to 28x28\n",
    "show_mnist(images[0].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# now, we can use the find_nearest neighbours\n",
    "# pick an image at random\n",
    "index = np.random.randint(0, len(images))\n",
    "img, lbl = images[index], labels[index]\n",
    "\n",
    "show_mnist(img.reshape(28, 28))\n",
    "\n",
    "nearest_neighbours = find_nearest(images, images[index], index, 10, manhattan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from this list, extract corresponding votes and tally it up!\n",
    "nn_labels = [labels[i] for i, d in nearest_neighbours]\n",
    "nn_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# there's a handy class called Counter we can use\n",
    "from collections import Counter\n",
    "counts = Counter(nn_labels)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "counts.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prediction = counts.most_common(1)[0][0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's pull all of this into a nice function\n",
    "def predict(index, images, k, distance_function):\n",
    "    nearest_neighbours = find_nearest(images, images[index], index, k, distance_function)\n",
    "    nn_labels = [labels[i] for i, d in nearest_neighbours]\n",
    "    counts = Counter(nn_labels)\n",
    "    return counts.most_common(1)[0][0]\n",
    "\n",
    "# this is for prediction if we have a new image\n",
    "def predict_image(img, images, labels, k, distance_function):\n",
    "    nearest_neighbours = find_nearest(images, img, -1, k, distance_function)\n",
    "    nn_labels = [labels[i] for i, d in nearest_neighbours]\n",
    "    counts = Counter(nn_labels)\n",
    "    return counts.most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "index = np.random.randint(0, len(images))\n",
    "img, lbl = images[index], labels[index]\n",
    "\n",
    "show_mnist(img.reshape(28, 28))\n",
    "predict(index, images, 10, manhattan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Evaluation of Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# assumption: let's assume there are only two classes - 0 and 1 \n",
    "# this is called a binary classification problem, btw\n",
    "\n",
    "# let's take these as examples\n",
    "y_actual =    [0,1,0,0,0,0,1,1,0,1]\n",
    "y_predicted = [1,0,0,0,0,1,1,1,0,1]\n",
    "\n",
    "# is this good / bad?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# simplest metric ever!\n",
    "def accuracy(actual, predicted):\n",
    "    assert len(predicted)  == len(actual)\n",
    "    correct = 0.0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == p:\n",
    "            correct += 1\n",
    "    return correct / len(actual)\n",
    "accuracy(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# problem with accuracy :(\n",
    "# let's say we have an unbalanced dataset in which almost all points are 1 \n",
    "unbalanced_y_actual = [1,1,1,1,1,1,1,1,0,1]\n",
    "# let's say we have a classifier which is a single line of code that predicts '1' all the time!\n",
    "unbalanced_y_predicted = [1,1,1,1,1,1,1,1,1,1]\n",
    "\n",
    "accuracy(unbalanced_y_actual, unbalanced_y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### To tackle this, we can come up with more metrics! \n",
    "Let's first create a helpful matrix of all possible results in a binary classification setting:\n",
    "\n",
    "||Labels| $0$  | $1$  | $Predicted$  |\n",
    "|---|---|---|---|\n",
    "|$Actual$| $0$     | $True Negative$  | $False Positive$  | \n",
    "|$Actual$| $1$     |  $False Negative$ |  $True Positive$ |  \n",
    "\n",
    "\n",
    "We can now define precision and recall:\n",
    "In a binary setting, \n",
    "\n",
    "Precision is the fraction of correctly predicted values - $1$s among all the values we've **predicted** as $1$. \n",
    "\n",
    "Recall is the fraction of correctly predicted values - $1$s over **total number** of $1$s instances in the image.\n",
    "\n",
    "\n",
    "Can you guess:\n",
    "\n",
    "$Precision = ?$\n",
    "\n",
    "$Recall = ?$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Precision = \\frac{TP}{(TP  + FP)}$\n",
    "\n",
    "$Recall = \\frac{TP}{(TP  + FN)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and recall intuition\n",
    "A system with high recall but low precision returns many results, but most of its predicted labels are incorrect when compared to the training labels.\n",
    "\n",
    "A system with high precision but low recall is just the opposite, returning very few results, but most of its predicted labels are correct when compared to the training labels.\n",
    "\n",
    "An ideal system with high precision and high recall will return many results, with all results labeled correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count true positives \n",
    "def true_positive(actual, predicted, pos_label):\n",
    "    tp = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == pos_label and a == p:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(actual, predicted, pos_label):\n",
    "    tn = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a != pos_label and a == p:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(actual, predicted, pos_label):\n",
    "    fp = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a != pos_label and p == pos_label:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(actual, predicted, pos_label):\n",
    "    fn = 0\n",
    "    for a, p in zip(actual, predicted):\n",
    "        if a == pos_label and p != pos_label:\n",
    "            fn += 1\n",
    "    return fn\n",
    "\n",
    "def confusion_matrix(actual, predicted, pos_label):\n",
    "    tp = true_positive(actual, predicted, pos_label)\n",
    "    tn = true_negative(actual, predicted, pos_label)\n",
    "    fp = false_positive(actual, predicted, pos_label)\n",
    "    fn = false_negative(actual, predicted, pos_label)\n",
    "    return [\n",
    "        [tn, fp],\n",
    "        [fn, tp]\n",
    "    ]\n",
    "\n",
    "def precision(actual, predicted, pos_label):\n",
    "    tp = true_positive(actual, predicted, pos_label)\n",
    "    fp = false_positive(actual, predicted, pos_label)\n",
    "    if tp == 0:\n",
    "        return 0\n",
    "    return float(tp) / (tp + fp)\n",
    "    \n",
    "    \n",
    "def recall(actual, predicted, pos_label):\n",
    "    # fill this up!\n",
    "    return 0\n",
    "\n",
    "print precision(y_actual, y_predicted, 1)\n",
    "print recall(y_actual, y_predicted, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print precision(unbalanced_y_actual, unbalanced_y_predicted, 1)\n",
    "print recall(unbalanced_y_actual, unbalanced_y_predicted, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print precision(unbalanced_y_actual, unbalanced_y_predicted, 0)\n",
    "print recall(unbalanced_y_actual, unbalanced_y_predicted, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# play around with Precision and Recall\n",
    "actual = [1, 1, 0, 0]\n",
    "predicted = [1, 1, 0, 1]\n",
    "print precision(actual, predicted, 1)\n",
    "print recall(actual, predicted, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One metric to rule them all!\n",
    "\n",
    "$F1$ score is the harmonic mean of Precision & Recall\n",
    "\n",
    "$F1 = 2\\frac{P . R}{P+R} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f1_score(actual, predicted, pos_label):\n",
    "    # fill this up!\n",
    "    return 0\n",
    "\n",
    "print \"Balanced: {}\".format(f1_score(actual, predicted, 1))\n",
    "print \"Unbalanced: {}\".format(f1_score(actual, predicted, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_all_metrics(y_actual, y_predicted):\n",
    "    for label in np.unique(y_actual):\n",
    "        print \"LABEL: {}\".format(label)\n",
    "        print \"\\tAccuracy: \", accuracy(y_actual, y_predicted)\n",
    "        print \"\\tTrue Positive: \", true_positive(y_actual, y_predicted, label)\n",
    "        print \"\\tTrue Negative: \", true_negative(y_actual, y_predicted, label)\n",
    "        print \"\\tFalse Positive: \", false_positive(y_actual, y_predicted, label)\n",
    "        print \"\\tFalse Negative: \", false_negative(y_actual, y_predicted, label)\n",
    "        print \"\\tConfusion Matrix: \", confusion_matrix(y_actual, y_predicted, label)\n",
    "        print \"\\tPrecision: \", precision(y_actual, y_predicted, label)\n",
    "        print \"\\tRecall: \", recall(y_actual, y_predicted, label)\n",
    "        print \"\\tF1 Score: \", f1_score(y_actual, y_predicted, label)\n",
    "        print\n",
    "print_all_metrics(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now let's take a look at the unbalanced example\n",
    "print_all_metrics(unbalanced_y_actual, unbalanced_y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's select a sample, because prediction on the whole data takes a loooong time!\n",
    "idx_ = np.random.permutation(range(0, len(images)))[:100]\n",
    "predicted_eval_sample = []\n",
    "for i in idx_:\n",
    "    predicted_eval_sample.append(predict(i, images, 10, euclidean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_all_metrics(labels[idx_], predicted_eval_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision vs Recall vs F1 Score \n",
    "\n",
    "Which is more imporant?\n",
    "\n",
    "- Spam\n",
    "- Biometric scanning\n",
    "- Credit card fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Generalization\n",
    "\n",
    "### Problem 1. How can you confirm whether your model works in practice? \n",
    "### Problem 2. How can I select *hyperparameters* for my model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(\"./sample_space.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can you confirm whether your model works in practice?\n",
    "### Solution: Separate your data into 2 sets - the train and the test set\n",
    "\n",
    "Call one a training set and use it for training your data. This is typically 60-80% of your data (depending on the # samples you have)\n",
    "The other set - the test set - is only touched *once* when you evaluate your *final* model. \n",
    "\n",
    "\n",
    "As long as the # samples in the test set is large enough (and r, we can safely assume that efficacy of the model in the test set is approximately equal to the efficacy of the model on the 'real world' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2. How can I select *hyperparameters* for my model?\n",
    "### Solution for #2: Separate your *train* data into 2 sets: the train and validation set. The test set remains untouched\n",
    "\n",
    "The first set, as the name suggests, will be used for training. This is typically 50-60% of the total data.\n",
    "The second set, will be used to tune hyperparameters. This is typically 10-20% of the total data.\n",
    "\n",
    "So in essense, there are 3 sets of data:\n",
    "- Train Set - 50-60% of the total data\n",
    "- Validaton Set - 10-20% of the total data\n",
    "- Test Set - 20-40% of the total data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "- Select a metric most important for you (ex precision)\n",
    "\n",
    "\n",
    "- For each set of parameters, fit your model\n",
    "- Evalute model using validation set\n",
    "- Store evaluation\n",
    "\n",
    "\n",
    "- Pick the best performing model on the validation set as your final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data \n",
    "total_size = len(images)\n",
    "\n",
    "train_size = int(0.6 * total_size)\n",
    "validation_size = 100 #int(0.2 * total_size)\n",
    "print train_size, validation_size, (total_size-train_size-validation_size)\n",
    "shuffle_idx = np.random.permutation(range(0, len(images)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train = images[shuffle_idx[0: train_size]], labels[shuffle_idx[0: train_size]]\n",
    "print x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_valid = images[shuffle_idx[train_size: train_size + validation_size]]\n",
    "y_valid = labels[shuffle_idx[train_size: train_size + validation_size]]\n",
    "print x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test = images[shuffle_idx[train_size + validation_size: ]]\n",
    "y_test = labels[shuffle_idx[train_size + validation_size: ]]\n",
    "print x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's pick precision as the metric\n",
    "selection_metric = precision\n",
    "\n",
    "\n",
    "parameter_grid = [\n",
    "    {\n",
    "        \"k\": 1,\n",
    "        \"distance\": euclidean\n",
    "    },\n",
    "    {\n",
    "        \"k\": 5,\n",
    "        \"distance\": euclidean\n",
    "    },\n",
    "    {\n",
    "        \"k\": 10,\n",
    "        \"distance\": euclidean\n",
    "    },\n",
    "    {\n",
    "        \"k\": 1,\n",
    "        \"distance\": manhattan\n",
    "    },\n",
    "    {\n",
    "        \"k\": 5,\n",
    "        \"distance\": manhattan\n",
    "    },\n",
    "    {\n",
    "        \"k\": 10,\n",
    "        \"distance\": manhattan\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "evaluated = []\n",
    "for param_dict in parameter_grid:\n",
    "    print \"Trying Parameter: {}\".format(param_dict)\n",
    "    predictions = []\n",
    "    # compute predictions for the validation set\n",
    "    for i, img in enumerate(x_valid):\n",
    "        # print a progress report once in a while\n",
    "        if i % 10 == :\n",
    "            print \"\\t{} of {} images\".format(i, len(x_valid))\n",
    "        # use only the train images and labels here! \n",
    "        predictions.append(predict_image(img, x_train, y_train, param_dict[\"k\"], param_dict[\"distance\"]))\n",
    "    \n",
    "    per_label_selection_metrics = []\n",
    "    for label in np.unique(labels[idx_]):\n",
    "        per_label_selection_metrics.append(selection_metric(y_valid, predictions, label))\n",
    "    evaluated.append({\n",
    "            \"params\" : param_dict,\n",
    "            \"metric\": np.mean(per_label_selection_metrics)\n",
    "        })\n",
    "    \n",
    "    \n",
    "for p in evaluated:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_params = max(evaluated, key=lambda _: _[\"metric\"])\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k = best_params[\"params\"][\"k\"]\n",
    "best_dist = best_params[\"params\"][\"distance\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just a reminder: *Never* touch the test set except only once in your entire process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_idx = 100\n",
    "y_test_pred = [predict_image(img, x_train, y_train, best_k, best_dist) for img in x_test[:t_idx]]\n",
    "\n",
    "test_eval = []\n",
    "for label in np.unique(labels):\n",
    "    test_eval.append(selection_metric(y_test[:t_idx], y_test_pred, label))\n",
    "    \n",
    "print np.mean(test_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with KNN\n",
    "\n",
    "Pros?\n",
    "Cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Summary\n",
    "\n",
    "**Data preparation and exploration** (more on this later)\n",
    "- Explore the data. Note any biases and properties of the data that may affect modeling and evaluation (skewed class distribution, high or low variance, etc).\n",
    "- Decide on a metric\n",
    "- Decide which algorithm(s) to use (more on this later)\n",
    "- Prepare dataset: Convert it into X & y matrices\n",
    "\n",
    "**Modeling Phase**\n",
    "- Split the dataset into a training, validation and test set (more on this later)\n",
    "- Train the models(s) on the training set and tune hyperparameters\n",
    "- Repeat until you get a good score on the validation set.\n",
    "\n",
    "**Test step**\n",
    "- Compute test accuracy. Remember this has to be done only once. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn\n",
    "\n",
    "Everything we just did can be reduced to a few lines of code if we use `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, DistanceMetric\n",
    "# data preparation\n",
    "labels, images = read_mnist_full()\n",
    "images = images.reshape(60000, 28*28)\n",
    "print labels.shape, images.shape\n",
    "\n",
    "# create train test split\n",
    "\n",
    "# 20% test set\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size=0.2)\n",
    "\n",
    "# get only the first 5000 images\n",
    "x_train, y_train = x_train[:5000], y_train[:5000]\n",
    "\n",
    "# get only the first 1000 images \n",
    "x_test, y_test = x_test[:1000], y_test[:1000]\n",
    "\n",
    "print \"Train:\", x_train.shape, y_train.shape\n",
    "print \"Test:\", x_test.shape, y_test.shape\n",
    "\n",
    "parameter_grid = {\n",
    "    \"metric\": [DistanceMetric.get_metric(\"euclidean\"), \n",
    "               DistanceMetric.get_metric(\"manhattan\")],\n",
    "    \"n_neighbors\": [1, 5, 10]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(KNeighborsClassifier(algorithm = 'ball_tree'), parameter_grid, \n",
    "             scoring=make_scorer(precision_score, average=\"macro\"),n_jobs=3, cv=2, verbose=2)\n",
    "clf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_score(y_test, clf.predict(x_test), average=\"macro\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
